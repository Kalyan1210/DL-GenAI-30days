### DayÂ 1Â Reflection

* Achieved 97.67â€¯% val accuracy in 5Â epochs
* Learned: tensors, autograd, basic training loop
* Roadblocks: initial import path and preâ€‘commit flow
* Next: manual backâ€‘prop sanity check

## Dayâ€¯1 â€” EnvironmentÂ + 2â€‘LayerÂ MLP

### ğŸš€ What I built
- Twoâ€‘layer MLP on MNIST (`src/train_mnist.py`, `src/models/mlp.py`)
- Achieved **97.67â€¯%** validation accuracy in 5â€¯epochs on âœ…Â device:Â <mps/cpu/cuda>

### ğŸ§  Key concepts learned
- Dynamic computation graph & leaf tensors
- Forwardâ€‘backwardâ€‘update training loop
- Device management (`mps`, `cuda`, `cpu`)
- Data pipeline with `torchvision.datasets` + `DataLoader`

### ğŸ› ï¸ Roadblocks / fixes
| Issue | Fix |
|-------|-----|
| `ModuleNotFoundError: models` | Added `src/__init__.py` and correct import path |
| Preâ€‘commit aborts | Staged rewritten files, reran commit |
| Large dataset push | Added `data/` to `.gitignore`, rewrote commit |

### â“ Open questions
- Why orthogonalâ€¯+â€¯gainâ€¯=â€¯âˆš2 is best for ReLU?
- When to switch from SGD to Adam in practice?

### ğŸ“Œ TODO tomorrow
- Manual backâ€‘prop sanity check (Dayâ€¯2 task)
- Try LRÂ finder script to verify `1eâ€‘3` is stable

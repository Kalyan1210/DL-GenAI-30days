## Dayâ€¯2 â€” Manual gradients

### âœ… Gradient check
| Param | Relative diff |
|-------|---------------|
| W1 | 8.49e-08 |
| b1 | 9.75e-08 |
| W2 | 7.95e-08 |
| b2 | 1.12e-07 |

### ğŸ’¡ One key thing I learned
<e.g. â€œSoftmax-cross-entropy simplifies gradient to probs âˆ’ one-hot.â€>

### âš ï¸ One roadblock / surprise
<e.g. â€œ`nn.Sequential` needs an nn.Module, so I had to wrap MyReLU.â€>

### ğŸ¤” Question to revisit
<e.g. â€œWhy does He init use âˆš2/ fan_in for ReLU?â€>

### ğŸ“ˆ Loss plot saved
Manual training loss curve: `manual_loss.png`
